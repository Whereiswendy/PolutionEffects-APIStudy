{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Data Extraction Process and Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Exploring processes and code to easily access data relevant to study/project.\n",
    "\n",
    "### Project Hypothesis:\n",
    "Socioeconomic status, as indicated by income levels, education attainment, and race/ethnicity, is a significant predictor of air quality and health outcomes. Communities with lower socioeconomic status are hypothesized to experience poorer air quality, which in turn leads to a higher prevalence of adverse health outcomes. This relationship is expected to persist even when controlling for potential confounding variables such as geographic location and access to healthcare services.\n",
    "\n",
    "### Defining Data Collection Parameters\n",
    "- **Geographic Scope:** Define countries or cities of interest\n",
    "- **Time Frame:** Define time period coverage\n",
    "- **Socioeconomic Indicators:** Define indicators of interest (e.g., median income, education level)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# Import the relevant API keys ( you will need )\n",
    "from api_keys import weather_api_key\n",
    "from api_keys import geoapify_key\n",
    "from api_keys import aqicn_api_key\n",
    "from api_keys import gho_who_api_key\n",
    "from api_keys import api_ninjas_key\n",
    "\n",
    "# Import citipy to determine the cities based on latitude and longitude\n",
    "from citipy import citipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographic scope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique cities in the list: 618\n"
     ]
    }
   ],
   "source": [
    "# Empty list for holding the latitude and longitude combinations\n",
    "lat_lngs = []\n",
    "\n",
    "# Empty dictionary for holding the city names and country codes\n",
    "city_details = {}\n",
    "\n",
    "# Range of latitudes and longitudes\n",
    "lat_range = (-90, 90) # Min and Max bounds for latitude range\n",
    "lng_range = (-180, 180) # Min and Max bounds for longitude range\n",
    "\n",
    "# Create a set of random lat and lng combinations\n",
    "lats = np.random.uniform(lat_range[0], lat_range[1], size=1500)\n",
    "lngs = np.random.uniform(lng_range[0], lng_range[1], size=1500)\n",
    "lat_lngs = zip(lats, lngs) # Aggregate into tuple - pairing latitudes and longitudes\n",
    "\n",
    "# Identify nearest city, country, and record their coordinates for each lat, lng combination\n",
    "for lat, lng in lat_lngs:\n",
    "    city = citipy.nearest_city(lat, lng)\n",
    "    city_name = city.city_name\n",
    "    country_code = city.country_code\n",
    "    coords = (lat, lng)\n",
    "    \n",
    "    # If the city is unique, then add it along with the country code and coordinates\n",
    "    if city_name not in city_details:\n",
    "        city_details[city_name] = (country_code, coords)\n",
    "\n",
    "# Print the city count to confirm sufficient count\n",
    "print(f\"Number of unique cities in the list: {len(city_details)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Index                         City Country  \\\n",
      "0        0                      malpica      es   \n",
      "1        1                       bethel      us   \n",
      "2        2               ribeira grande      pt   \n",
      "3        3  edinburgh of the seven seas      sh   \n",
      "4        4                       albany      au   \n",
      "..     ...                          ...     ...   \n",
      "613    613                       murzuq      ly   \n",
      "614    614             charlotte amalie      vi   \n",
      "615    615                     kangding      cn   \n",
      "616    616                    dingcheng      cn   \n",
      "617    617                    groningen      sr   \n",
      "\n",
      "                                        Coords  \n",
      "0     (46.058636950605745, -9.777245982901945)  \n",
      "1      (68.68194407511422, -164.0566600183777)  \n",
      "2      (48.51075858434373, -32.88817223429896)  \n",
      "3     (-37.98593329638902, -1.392334171241373)  \n",
      "4    (-58.607549635501265, 124.70233480599364)  \n",
      "..                                         ...  \n",
      "613    (23.98361572585003, 13.641477037660138)  \n",
      "614    (18.714373021089727, -65.0067445411061)  \n",
      "615   (32.785853194924485, 101.23845273354056)  \n",
      "616   (32.385756595195446, 115.15736770440822)  \n",
      "617    (5.575780986260668, -55.89521675244734)  \n",
      "\n",
      "[618 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the collected data\n",
    "cities_selected_df = pd.DataFrame({\n",
    "    'City': [k for k in city_details.keys()],\n",
    "    'Country': [v[0] for v in city_details.values()],\n",
    "    'Coords': [v[1] for v in city_details.values()]\n",
    "})\n",
    "\n",
    "# Reset index to make sure it starts from 0 and acts as an index column\n",
    "cities_selected_df.reset_index(inplace=True)\n",
    "cities_selected_df.rename(columns={'index': 'Index'}, inplace=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(cities_selected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Socio-economic indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is the code used - it uses about 5% of the month allocation of 10000 requests. One can batch into groups of 30 to reduce the number of requests.\n",
    "\n",
    "cities_selected_df1=cities_selected_df.copy()\n",
    "\n",
    "headers = {'X-Api-Key': api_ninjas_key}\n",
    "\n",
    "# Empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Function to fetch data for a city\n",
    "def get_city_data(city):\n",
    "    url = f'https://api.api-ninjas.com/v1/city?name={city}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {city}, status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Loop through the DataFrame and call the API for each city\n",
    "for row in cities_selected_df1.itertuples():\n",
    "    # Convert city names to Title Case\n",
    "    city_name_formatted = row.City.title()  # Corrected to use 'City' instead of 'city_name'\n",
    "    data = get_city_data(city_name_formatted)\n",
    "    if data:\n",
    "        results.extend(data)  # Extend in case data is a list of multiple cities\n",
    "    # Implement a delay between API calls to avoid hitting the rate limit\n",
    "    time.sleep(1)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_cities_results = pd.DataFrame(results)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists('Data'):\n",
    "    os.makedirs('Data')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_cities_results.to_csv(\"Data/city_data_ww.csv\", index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'is_rural' variable to the dataframe\n",
    "\n",
    "# 1. Make a copy of the DataFrame\n",
    "df_cities_results1 = df_cities_results.copy()\n",
    "\n",
    "# 2. Generate a column called is_rural\n",
    "# Assuming the population column is named \"Population\"\n",
    "df_cities_results1['is_rural'] = df_cities_results1['population'] < 15000  # This will return True for rural and False for urban\n",
    "\n",
    "# 3. Loop through the populations against measures\n",
    "# This step is not needed as the vectorized operation above handles the classification based on population\n",
    "\n",
    "# 4. Output to \"Data/city_data.csv\"\n",
    "df_cities_results1.to_csv(\"Data/city_data1_ww.csv\", index=False)  # index=False to avoid writing row indices in the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQICN API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'data': {'aqi': 28,\n",
       "  'idx': 12842,\n",
       "  'attributions': [{'url': 'https://www.irceline.be/en/',\n",
       "    'name': 'IRCEL-CELINE - Belgian Interregional Environment Agency',\n",
       "    'logo': 'Beligium-irceline.png'},\n",
       "   {'url': 'https://waqi.info/', 'name': 'World Air Quality Index Project'}],\n",
       "  'city': {'geo': [50.462427939605, 4.8652609460116],\n",
       "   'name': 'Namur, Belgium',\n",
       "   'url': 'https://aqicn.org/city/belgium/wal/namur',\n",
       "   'location': ''},\n",
       "  'dominentpol': 'pm25',\n",
       "  'iaqi': {'h': {'v': 82},\n",
       "   'p': {'v': 1003.3},\n",
       "   'pm10': {'v': 14},\n",
       "   'pm25': {'v': 28},\n",
       "   't': {'v': 8.3},\n",
       "   'w': {'v': 1.5},\n",
       "   'wg': {'v': 5.5}},\n",
       "  'time': {'s': '2024-04-26 10:00:00',\n",
       "   'tz': '+02:00',\n",
       "   'v': 1714125600,\n",
       "   'iso': '2024-04-26T10:00:00+02:00'},\n",
       "  'forecast': {'daily': {'o3': [{'avg': 31,\n",
       "      'day': '2024-04-25',\n",
       "      'max': 39,\n",
       "      'min': 21},\n",
       "     {'avg': 29, 'day': '2024-04-26', 'max': 39, 'min': 24},\n",
       "     {'avg': 27, 'day': '2024-04-27', 'max': 36, 'min': 20},\n",
       "     {'avg': 34, 'day': '2024-04-28', 'max': 38, 'min': 31},\n",
       "     {'avg': 29, 'day': '2024-04-29', 'max': 29, 'min': 21}],\n",
       "    'pm10': [{'avg': 12, 'day': '2024-04-25', 'max': 15, 'min': 8},\n",
       "     {'avg': 12, 'day': '2024-04-26', 'max': 14, 'min': 10},\n",
       "     {'avg': 6, 'day': '2024-04-27', 'max': 14, 'min': 3},\n",
       "     {'avg': 4, 'day': '2024-04-28', 'max': 6, 'min': 2},\n",
       "     {'avg': 6, 'day': '2024-04-29', 'max': 6, 'min': 6}],\n",
       "    'pm25': [{'avg': 39, 'day': '2024-04-25', 'max': 45, 'min': 28},\n",
       "     {'avg': 47, 'day': '2024-04-26', 'max': 53, 'min': 39},\n",
       "     {'avg': 19, 'day': '2024-04-27', 'max': 53, 'min': 9},\n",
       "     {'avg': 13, 'day': '2024-04-28', 'max': 19, 'min': 6},\n",
       "     {'avg': 17, 'day': '2024-04-29', 'max': 18, 'min': 16}]}},\n",
       "  'debug': {'sync': '2024-04-26T17:29:51+09:00'}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_selected_df2=df_cities_results1.copy()\n",
    "\n",
    "# Define the endpoint and parameters for your request\n",
    "endpoint = 'http://api.waqi.info/feed/'\n",
    "city = 'name'\n",
    "params = {\n",
    "    'token': aqicn_api_key\n",
    "}\n",
    "\n",
    "# Make the request and collect the data\n",
    "response = requests.get(f'{endpoint}/{city}/', params=params)\n",
    "data = response.json()\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   so2  aqi  dew  pm10   r  uvi   d  wd  co.v   h.v  ...  pm10.v   h   p   t  \\\n",
      "0  NaN  NaN  NaN   NaN NaN  NaN NaN NaN   3.2   3.0  ...     NaN NaN NaN NaN   \n",
      "1  NaN  NaN  NaN   NaN NaN  NaN NaN NaN   2.3  60.3  ...     NaN NaN NaN NaN   \n",
      "2  NaN  NaN  NaN   NaN NaN  NaN NaN NaN   2.7  84.5  ...     NaN NaN NaN NaN   \n",
      "3  NaN  NaN  NaN   NaN NaN  NaN NaN NaN   NaN  66.0  ...     NaN NaN NaN NaN   \n",
      "4  NaN  NaN  NaN   NaN NaN  NaN NaN NaN   NaN  97.0  ...    58.0 NaN NaN NaN   \n",
      "\n",
      "    w  r.v  uvi.v  d.v  wd.v        city  \n",
      "0 NaN  NaN    NaN  NaN   NaN      bethel  \n",
      "1 NaN  NaN    NaN  NaN   NaN      albany  \n",
      "2 NaN  NaN    NaN  NaN   NaN    hamilton  \n",
      "3 NaN  NaN    NaN  NaN   NaN  georgetown  \n",
      "4 NaN  NaN    NaN  NaN   NaN      kourou  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_cities_results1 is the original DataFrame\n",
    "df_city_results_aqicn_ww = cities_selected_df.copy()\n",
    "\n",
    "# Define the endpoint and the API key\n",
    "endpoint = 'http://api.waqi.info/feed/'\n",
    "api_key = aqicn_api_key  # Make sure to replace this with your actual API key\n",
    "\n",
    "# Prepare to collect the AQI data\n",
    "aqi_data = []\n",
    "\n",
    "# Loop through each city in the DataFrame\n",
    "for city in df_city_results_aqicn_ww['City']:\n",
    "    # Make the request and collect the data\n",
    "    response = requests.get(f'{endpoint}/{city}/', params={'token': api_key})\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if the response is successful and data is available\n",
    "    if data['status'] == 'ok' and 'data' in data and 'iaqi' in data['data']:\n",
    "        # Extract air quality information\n",
    "        aq_data = data['data']['iaqi']\n",
    "        aq_data['city'] = city  # Add the city name to the dictionary\n",
    "        aqi_data.append(aq_data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_aq = pd.DataFrame.from_records(aqi_data)\n",
    "\n",
    "# If you want each pollutant in its own column, you may need to transform the data\n",
    "# Normalize the data to expand the nested dictionaries into their own columns\n",
    "df_aqicn_expanded = pd.json_normalize(df_aq.drop(columns=['city']).to_dict(orient='records'))\n",
    "df_aqicn_expanded['city'] = df_aq['city']  # Add the city column back after normalization\n",
    "\n",
    "# Output the DataFrame to verify\n",
    "print(df_aqicn_expanded.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df_aqicn_expanded.to_csv('Data/aqi_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenWeatherMap API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "df_city_results_openweather=df_cities_results.copy()\n",
    "\n",
    "# API endpoint and your OpenWeatherMap API key\n",
    "api_key = weather_api_key\n",
    "endpoint = 'http://api.openweathermap.org/data/2.5/air_pollution'\n",
    "\n",
    "# Prepare to collect the air pollution data\n",
    "air_pollution_data = []\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df_city_results_openweather.iterrows():\n",
    "    # Extract latitude and longitude\n",
    "    lat, lon = row['latitude'], row['longitude']\n",
    "    \n",
    "    # Make the request\n",
    "    response = requests.get(f\"{endpoint}?lat={lat}&lon={lon}&appid={api_key}\")\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if the response is successful and data is available\n",
    "    if response.status_code == 200 and 'list' in data:\n",
    "        # Extract air pollution details (taking the first item in 'list' as example)\n",
    "        pollution_info = data['list'][0]\n",
    "        \n",
    "        # Add the pollution data to the DataFrame row\n",
    "        for key, value in pollution_info.items():\n",
    "            if isinstance(value, dict):  # This check is to flatten nested dictionaries\n",
    "                for subkey, subvalue in value.items():\n",
    "                    df_city_results_openweather.at[index, f\"{key}_{subkey}\"] = subvalue\n",
    "            else:\n",
    "                df_city_results_openweather.at[index, key] = value\n",
    "\n",
    "# Optional: Drop the original latitude and longitude columns if you don't want them duplicated\n",
    "# df = df.drop(columns=['latitude', 'longitude'])\n",
    "\n",
    "# Save the DataFrame with air pollution data to a CSV file\n",
    "df_city_results_openweather.to_csv('Data/combined_city_pollution_data.csv', index=False)\n",
    "\n",
    "print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
